1000: 0.3786
5000: 0.4240
10000: 0.4356
15000: 0.4434
20000: 0.4475
Evidently, the more data has a significant impact on themodel performance, with a strict increase in the test accuracy as the training data increases. Interestingly, There is not much improvement between 15k to 20k samples, likely because we are hitting the limit capacity for thedefault 50 classifiers in the Adaboost classifier (best).Regardless, the accuracies are quite low across all datasetsizes, likely due to the variance captured in the training data. Generally, these descriptive features like age of acquisition, etc., might not well relate with the actual categorization of left, right, alt, and center. An improvementmight be to use a higher-capacity model, with lots more raw data. I would suggest raw data because, with enough of it, and a good learning algorithm, the model can learn from the textthe important features, rather than us guessing what will explain the correlation best. Of course, we could also try other features that are better tailored to enable the models to learn the correlation between input and output.