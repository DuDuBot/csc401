Results for SGDClassifier:
	Accuracy: 0.3796
	Recall: [0.569, 0.1355, 0.734, 0.08]
	Precision: [0.5471, 0.4033, 0.3042, 0.3783]
	Confusion Matrix: 
[[1138   50  745   67]
 [ 332  271 1296  101]
 [ 283  154 1468   95]
 [ 327  197 1316  160]]

Results for GaussianNB:
	Accuracy: 0.3292
	Recall: [0.8605, 0.2595, 0.156, 0.041]
	Precision: [0.3222, 0.3167, 0.4041, 0.3306]
	Confusion Matrix: 
[[1721  147  104   28]
 [1247  519  170   64]
 [1078  536  312   74]
 [1295  437  186   82]]

Results for RandomForestClassifier:
	Accuracy: 0.4261
	Recall: [0.6295, 0.3035, 0.4775, 0.294]
	Precision: [0.5344, 0.3803, 0.3919, 0.365]
	Confusion Matrix: 
[[1259  204  320  217]
 [ 373  607  549  471]
 [ 326  384  955  335]
 [ 398  401  613  588]]

Results for MLPClassifier:
	Accuracy: 0.3837
	Recall: [0.4115, 0.2055, 0.1875, 0.7305]
	Precision: [0.606, 0.3967, 0.5137, 0.2996]
	Confusion Matrix: 
[[ 823  140   74  963]
 [ 195  411  143 1251]
 [ 186  238  375 1201]
 [ 154  247  138 1461]]

Results for AdaBoostClassifier:
	Accuracy: 0.4542
	Recall: [0.666, 0.3925, 0.4845, 0.274]
	Precision: [0.5497, 0.403, 0.4336, 0.3931]
	Confusion Matrix: 
[[1332  211  288  169]
 [ 366  785  471  378]
 [ 342  390  969  299]
 [ 383  562  507  548]]

we see that the RandomForestClassifier and the AdaBoost classifiers perform the best on this data. RandomForest and AdaBoost are expected because these are both ensemble-based models (they aggregrate many weak-learners to reduce variance). The reduced helps these models generalize better, especially in low data regimes. The MLP model, being a neural architecture, is expected to perform a bit worse as they generally require lots of data to perform well. The MLP structure chosen by SKLearn is likely a simple MLP that does not require much data, allowing it to perform despite this low-data regime. The SGDClassifier uses a linear SVM given the default configuration which is surprising that it does not perform well, since they generally perform well despite low-data (since they learn a simple margin from support vectors). Finally, the GaussianNB likely suffers despite NB's relative success in simple NKP problems because text is often not modelled well using a Gaussian distribution.
